# æ–°ã—ã„éƒ½é“åºœçœŒã®è¿½åŠ æ–¹æ³•ï¼ˆæ±äº¬éƒ½ã®ä¾‹ï¼‰

æ–°ã—ã„éƒ½é“åºœçœŒã‚’è¿½åŠ ã™ã‚‹éš›ã®æ‰‹é †ã¨ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚

## ğŸ“‹ Firestoreãƒ‡ãƒ¼ã‚¿å‹ã«ã¤ã„ã¦

### âœ… ãƒ‡ãƒ¼ã‚¿å‹ã¯å®Œå…¨ã«å…±é€šåŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼

**å®šç¾©å ´æ‰€**: `src/features/scraping/domain/models.py`

- `Shop`ã‚¯ãƒ©ã‚¹ï¼ˆLine 10-145ï¼‰
- `to_firestore_dict()`ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆLine 65-92ï¼‰- **ã™ã¹ã¦ã®éƒ½é“åºœçœŒã§å…±é€š**
- `from_firestore_dict()`ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆLine 94-122ï¼‰- **ã™ã¹ã¦ã®éƒ½é“åºœçœŒã§å…±é€š**

**é‡è¦**: æ–°ã—ã„éƒ½é“åºœçœŒã‚’è¿½åŠ ã™ã‚‹éš›ã€`Shop`ã‚¯ãƒ©ã‚¹ã‚„`to_firestore_dict()`ã¯**å¤‰æ›´ä¸è¦**ã§ã™ã€‚

### Firestoreã‚¹ã‚­ãƒ¼ãƒ

```json
{
  "shop_id": "13_00001",
  "prefecture_code": "13",
  "prefecture_name": "æ±äº¬éƒ½",
  "name": "åº—å",
  "address": "ä½æ‰€",
  "phone": "é›»è©±ç•ªå·",
  "business_hours": "å–¶æ¥­æ™‚é–“",
  "closed_days": "å®šä¼‘æ—¥",
  "detail_url": "https://...",
  "website": "https://...",
  "benefits": "å„ªå¾…å†…å®¹",
  "description": "ç´¹ä»‹ã‚³ãƒ¡ãƒ³ãƒˆ",
  "parking": "é§è»Šå ´æƒ…å ±",
  "latitude": 35.6895,
  "longitude": 139.6917,
  "geocoded_at": "2025-01-15T12:00:00Z",
  "scraped_at": "2025-01-15T12:00:00Z",
  "updated_at": "2025-01-15T12:00:00Z",
  "is_active": true
}
```

ã™ã¹ã¦ã®éƒ½é“åºœçœŒã§**åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

---

## ğŸ¯ è¿½åŠ æ‰‹é †

### å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ3ã¤ã®ã¿ï¼‰

1. **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«**: `src/features/scraping/config/prefectures/tokyo.yaml` âœ… ä½œæˆæ¸ˆã¿
2. **ãƒ‘ãƒ¼ã‚µãƒ¼**: `src/features/scraping/parsers/prefectures/tokyo_parser.py` âœ… ä½œæˆæ¸ˆã¿
3. **ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼**: `src/features/scraping/scrapers/prefectures/tokyo.py` âœ… ä½œæˆæ¸ˆã¿

---

## ã‚¹ãƒ†ãƒƒãƒ—1: ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã«è¿½åŠ 

### `src/features/batch/orchestrator.py`

```python
# ===== ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ï¼ˆimportéƒ¨åˆ†ï¼‰=====
# TODO: æ±äº¬éƒ½ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ..scraping.scrapers.prefectures.ibaraki import IbarakiScraper
from ..scraping.scrapers.prefectures.tokyo import TokyoScraper  # è¿½åŠ 


# ===== ã‚¯ãƒ©ã‚¹ã®ä¸­ï¼ˆæ—¢å­˜ã®run_ibaraki_scrapingã®å¾Œã«è¿½åŠ ï¼‰=====
class BatchOrchestrator:
    # ... æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ ...

    def run_ibaraki_scraping(self) -> None:
        """èŒ¨åŸçœŒã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œ"""
        # ... æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ ...

    # TODO: æ±äº¬éƒ½ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
    def run_tokyo_scraping(self) -> None:
        """æ±äº¬éƒ½ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œ"""
        logger.info("Starting Tokyo scraping job")

        # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
        http_client = HTTPClient(
            timeout=self.settings.scraping_timeout,
            max_retries=self.settings.scraping_retry,
            user_agent=self.settings.scraping_user_agent,
        )

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½œæˆ
        scraper = TokyoScraper(http_client=http_client)

        # ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œ
        job = PrefectureScrapingJob(
            scraper=scraper,
            geocoding_service=self.geocoding_service,
            shop_repository=self.shop_repository,
            history_repository=self.history_repository,
            progress_repository=self.progress_repository,
            slack_notifier=self.slack_notifier,
        )

        result = job.execute()

        logger.info(f"Tokyo scraping job completed: {result.status.value}")

    def run_prefecture_scraping(self, prefecture_code: str) -> None:
        """æŒ‡å®šã•ã‚ŒãŸéƒ½é“åºœçœŒã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œ"""
        logger.info(f"Starting scraping job for prefecture: {prefecture_code}")

        if prefecture_code == "08":
            self.run_ibaraki_scraping()
        # TODO: æ±äº¬éƒ½ã‚’è¿½åŠ 
        elif prefecture_code == "13":
            self.run_tokyo_scraping()
        else:
            raise ValueError(
                f"Unsupported prefecture code: {prefecture_code}. "
                f"Currently, only Ibaraki (08) and Tokyo (13) are supported."  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚æ›´æ–°
            )
```

---

## ã‚¹ãƒ†ãƒƒãƒ—2: éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã®å®šç¾©ã‚’ç¢ºèª

### `src/features/scraping/domain/enums.py`

éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªï¼š

```python
class PrefectureCode(Enum):
    """éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰"""

    IBARAKI = ("08", "èŒ¨åŸçœŒ", "ibaraki")
    # TODO: æ±äº¬éƒ½ã‚’è¿½åŠ ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    TOKYO = ("13", "æ±äº¬éƒ½", "tokyo")
```

â€» é€šå¸¸ã¯è¿½åŠ ä¸è¦ã§ã™ãŒã€å‹ãƒã‚§ãƒƒã‚¯ã‚’å³å¯†ã«ã™ã‚‹å ´åˆã¯è¿½åŠ ã—ã¾ã™ã€‚

---

## ã‚¹ãƒ†ãƒƒãƒ—3: ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

### `.env.development`

```bash
# TODO: æ±äº¬éƒ½ã‚’å¯¾è±¡éƒ½é“åºœçœŒã«è¿½åŠ 
TARGET_PREFECTURES=08,13  # èŒ¨åŸçœŒã¨æ±äº¬éƒ½
```

### `.env.example`

æœ¬ç•ªç’°å¢ƒç”¨ã«ã‚‚åŒæ§˜ã«è¿½åŠ ï¼š

```bash
TARGET_PREFECTURES=08,13
```

---

## ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

### ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ãƒ†ã‚¹ãƒˆ

```bash
# Firestoreã‚¨ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’èµ·å‹•
make dev-start

# æ±äº¬éƒ½ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ
python scripts/run_scraping.py --prefecture 13 --debug
```

### ãƒ‡ãƒãƒƒã‚°

1. **VSCode**: ãƒ‡ãƒãƒƒã‚°ãƒ‘ãƒãƒ«ã§ "Scrape Ibaraki (Debug)" ã®è¨­å®šã‚’è¤‡è£½
2. **PyCharm**: Run Configuration ã‚’è¤‡è£½
3. å¼•æ•°ã‚’ `--prefecture 13` ã«å¤‰æ›´

---

## ã‚¹ãƒ†ãƒƒãƒ—5: è¨­å®šã®èª¿æ•´ï¼ˆTODOã‚’åŸ‹ã‚ã‚‹ï¼‰

### `tokyo.yaml`ã®èª¿æ•´

å®Ÿéš›ã®æ±äº¬éƒ½ã®ã‚µã‚¤ãƒˆã‚’èª¿æŸ»ã—ã¦ã€ä»¥ä¸‹ã‚’è¨­å®šï¼š

```yaml
scraping:
  # TODO: å®Ÿéš›ã®URLã«å¤‰æ›´
  base_url: "https://actual-tokyo-kosodate-site.jp"

  # TODO: æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºèª
  encoding: "utf-8"  # ã¾ãŸã¯ "shift_jis"

  urls:
    # TODO: å®Ÿéš›ã®URLæ§‹é€ ã«åˆã‚ã›ã‚‹
    list_page: "/list?page={page}"
    detail_pattern: "/detail/\\d+"

  pagination:
    # TODO: ç·ãƒšãƒ¼ã‚¸æ•°ã‚’ç¢ºèªã€ã¾ãŸã¯ auto_detect: true ã«è¨­å®š
    start_page: 1
    end_page: 50
    auto_detect: false
```

### `tokyo_parser.py`ã®èª¿æ•´

å®Ÿéš›ã®HTMLã‚’è¦‹ãªãŒã‚‰ã€ã‚»ãƒ¬ã‚¯ã‚¿ã‚’èª¿æ•´ï¼š

```python
def _extract_shop_name(self, soup: BeautifulSoup) -> Optional[str]:
    """åº—èˆ—åã‚’æŠ½å‡º"""
    # TODO: å®Ÿéš›ã®HTMLã«åˆã‚ã›ã¦ã‚»ãƒ¬ã‚¯ã‚¿ã‚’å¤‰æ›´
    name_tag = soup.select_one("h2.actual-class-name")
    if name_tag:
        return normalize_text(name_tag.get_text())
    return None

def _extract_field(self, soup: BeautifulSoup, label: str) -> Optional[str]:
    """ãƒ©ãƒ™ãƒ«ã«å¯¾å¿œã™ã‚‹å€¤ã‚’æŠ½å‡º"""
    # TODO: å®Ÿéš›ã®HTMLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«åˆã‚ã›ã¦å®Ÿè£…
    # ãƒ‘ã‚¿ãƒ¼ãƒ³A, B, C ã®ã„ãšã‚Œã‹ã‚’é¸æŠã€ã¾ãŸã¯ç‹¬è‡ªå®Ÿè£…
```

---

## ğŸ“ ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

æ–°ã—ã„éƒ½é“åºœçœŒã‚’è¿½åŠ ã™ã‚‹éš›ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼š

### 1. ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
- [ ] `src/features/scraping/config/prefectures/tokyo.yaml`
- [ ] `src/features/scraping/parsers/prefectures/tokyo_parser.py`
- [ ] `src/features/scraping/scrapers/prefectures/tokyo.py`

### 2. ã‚³ãƒ¼ãƒ‰ä¿®æ­£
- [ ] `src/features/batch/orchestrator.py` - ã‚¤ãƒ³ãƒãƒ¼ãƒˆè¿½åŠ 
- [ ] `src/features/batch/orchestrator.py` - `run_tokyo_scraping()` è¿½åŠ 
- [ ] `src/features/batch/orchestrator.py` - `run_prefecture_scraping()` ã«åˆ†å²è¿½åŠ 

### 3. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª¿æ•´
- [ ] `tokyo.yaml` - URLã¨ã‚»ãƒ¬ã‚¯ã‚¿ã‚’å®Ÿéš›ã®ã‚µã‚¤ãƒˆã«åˆã‚ã›ã‚‹
- [ ] `.env.development` - `TARGET_PREFECTURES` ã«è¿½åŠ 
- [ ] `.env.example` - `TARGET_PREFECTURES` ã«è¿½åŠ 

### 4. ãƒ‘ãƒ¼ã‚µãƒ¼èª¿æ•´
- [ ] `tokyo_parser.py` - `_extract_shop_name()` ã®ã‚»ãƒ¬ã‚¯ã‚¿èª¿æ•´
- [ ] `tokyo_parser.py` - `_extract_field()` ã®HTMLæ§‹é€ ã«åˆã‚ã›ã‚‹

### 5. ãƒ†ã‚¹ãƒˆ
- [ ] ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
- [ ] Firestore UIã§ãƒ‡ãƒ¼ã‚¿ç¢ºèª
- [ ] ã™ã¹ã¦ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒæ­£ã—ãæŠ½å‡ºã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª

### 6. ãƒ‡ãƒ—ãƒ­ã‚¤
- [ ] GitHubã«push
- [ ] Stagingç’°å¢ƒã§å‹•ä½œç¢ºèª
- [ ] æœ¬ç•ªç’°å¢ƒã¸ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆã‚¿ã‚°ã‚’åˆ‡ã‚‹ï¼‰

---

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹

1. `tokyo_parser.py`ã®`_extract_field()`ã‚’èª¿æ•´
2. å®Ÿéš›ã®HTMLã‚’`curl`ã§å–å¾—ã—ã¦ç¢ºèªï¼š
   ```bash
   curl "https://actual-site.jp/detail/123" -o test.html
   ```
3. BeautifulSoupã§ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ãƒ†ã‚¹ãƒˆï¼š
   ```python
   from bs4 import BeautifulSoup
   soup = BeautifulSoup(open("test.html"), "html.parser")
   print(soup.select("h2.shop-title"))
   ```

### ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒˆãƒ¼ã‚¯ãƒ³ã‚¨ãƒ©ãƒ¼

1. `tokyo.yaml`ã®`session.required`ã‚’ç¢ºèª
2. ä¸è¦ãªå ´åˆã¯`false`ã«è¨­å®š
3. å¿…è¦ãªå ´åˆã¯`tokyo.py`ã®`_init_session()`ã‚’å®Ÿè£…

### Firestoreã«ä¿å­˜ã•ã‚Œãªã„

1. `Shop`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒæ­£ã—ãä½œæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
2. `to_firestore_dict()`ã¯è‡ªå‹•çš„ã«å‘¼ã°ã‚Œã‚‹ã®ã§å¤‰æ›´ä¸è¦
3. ãƒ­ã‚°ã‚’ç¢ºèªï¼š`save_batch()`ã®ãƒ­ã‚°ã‚’è¦‹ã‚‹

---

## ğŸ’¡ ãƒ’ãƒ³ãƒˆ

### å…±é€šåŒ–ã•ã‚Œã¦ã„ã‚‹éƒ¨åˆ†ï¼ˆå¤‰æ›´ä¸è¦ï¼‰

- `Shop`ã‚¯ãƒ©ã‚¹ã®å®šç¾©
- `to_firestore_dict()`ãƒ¡ã‚½ãƒƒãƒ‰
- `ShopRepository.save_batch()`
- `PrefectureScrapingJob.execute()`
- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—

### éƒ½é“åºœçœŒã”ã¨ã«å¤‰ã‚ã‚‹éƒ¨åˆ†ï¼ˆå®Ÿè£…ãŒå¿…è¦ï¼‰

- URLæ§‹é€ ï¼ˆ`tokyo.yaml`ï¼‰
- HTMLã‚»ãƒ¬ã‚¯ã‚¿ï¼ˆ`tokyo_parser.py`ï¼‰
- ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ‰ç„¡ï¼ˆ`tokyo.py`ã®`_init_session()`ï¼‰

---

## ğŸ“š å‚è€ƒè³‡æ–™

- [èŒ¨åŸçœŒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼](../src/features/scraping/scrapers/prefectures/ibaraki.py) - å®Ÿè£…ã®å‚è€ƒ
- [èŒ¨åŸçœŒãƒ‘ãƒ¼ã‚µãƒ¼](../src/features/scraping/parsers/prefectures/ibaraki_parser.py) - å®Ÿè£…ã®å‚è€ƒ
- [ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºã‚¬ã‚¤ãƒ‰](LOCAL_DEVELOPMENT.md) - ãƒ‡ãƒãƒƒã‚°æ–¹æ³•
